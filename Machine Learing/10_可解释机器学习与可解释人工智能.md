#### 10.可解释机器学习与可解释人工智能（Explainable Machine Learning and Explainable Artficial Intelligence, Explainable ML and Explainable AI）

* ##### 10.1 Explainable/Interpretable ML

  * 概述

    * Local Explanation

    * Global Explanation

      

    * 用机器来协助判断履历

    * 用机器协助判断罪犯是否可以假释

    * 金融相关的决策常常依法需要提供理由

    * 模型诊断：到底机器学到了什么

  * Interpretable v.s. Powerful

    * 有些模型本质上是可解释的，例如：线性模型（体重、其他特点）
    * 深度网络是很难去解释的，因为它是一个黑盒，我们不使用它，但是它比线性模型更有用，所以我们应该让深度网络变成可解释的
    * Decision tree ：可解释、功能更强大

  * Local Explanation : Explain the Decision

    * 定义
      * Object x 	~	 Components : {x_1, ..., x_n, ..., x_N}
      * 我们想知道做决定时每个因素的重要性
      * Idea : 删除或修改影响因素的值，观察决定的变化
      * 大的决定变化    ~    重要的影响因素
    * 梯度限制的基本方法
      * 梯度饱和
      * 集成梯度
    * 对抗性解释

  * Global Explanation : Explain the whole Model

    * 激活最大化 (Activation Maximization)

      Input ~ Convalution ~ Max Pooling ~ Convalution ~ Max Pooling ~ Flatten ~ y_i (Digit)

      $\large x^* = arg\ max_x\ y_i$

      ![avatar](./images/u101_Activation_Maximization.png)

  * “Regularization” from Generator

    ![avatar](./images/u101_Generator.png)

  * Using a model to explain another

    * 一些模型很容易被解释

    * 使用可解释的模型模仿不可解释的模型

    * 线性模型无法模仿神经网络，然而它能模仿一个 local region

      

    * Local Interpretable Model - Agnostic Explanations (LIME)

      ![avatar](./images/u101_LIME.png)

    * LIME - Image

      ![avatar](./images/u101_LIME_Image_1.png)

      ![avatar](./images/u101_LIME_Image_2.png)

      ![avatar](./images/u101_LIME_Image_3.png)

    * Decision Tree - Tree regularization

      ![avatar](./images/u101_Tree_regularization.png)

---

* **10.2 Explainable AI**
  * 解释一个训练模型：共同的工具和应用
    * 属性 Attribution
      * Local v.s. Global attribution
        * Local attribution
        * Global attribution
          * 属性汇总输出
          * 实现：层间相关性传播 (Layer-wise relevance propagation, LRP)
            * 定位直接父节点
            * 计算贡献
            * 重新分配的输出
            * 向后传播直到到达输入
        
      * 完整性 Completeness

        * Flexible baseline - Integrated gradient

          * 似乎是复杂的，事实上实现比 DeepLIFT 更容易

          * 但是更慢
            $$
            \large (x_i - \widehat{x_i}) · \int ^1_{\alpha} \frac{\part S_c(\widehat{x})}{\part(\widehat{x_i})}|_{\widehat{x}=\widehat{x}+\alpha(x-\widehat{x})} {\rm d}\alpha
            $$

        * Fixed-0 baseline - LRP when no bias

        * Attributions sum up to current output - baseline output

        * Sensitivity-n 

      * 评价 Evaluation

        * 重心
        * 累积分数

    * 探索 Probing

      * Background
        * BERT architecture
        * BERT training
          * 标记语言模型
          * 预测下个句子
      * BERT 
        * learn
          * Probing tasks
          * Classifier - Edge probing technique
        * not learn

    * 热图 Heatmap
  * 创建一个可解释模型：重点在 CNN
    * 限制激活地图 Constraining activation maps
    * 编码优先 Encoding prior







































































