#### 12.无监督学习（Unsupervised Learning）

* ##### 12.1 字嵌入 (Word Embedding)

  * 类比

    * 1-of-N Encoding

      apple      = [1 0 0 0 0]

      bag          = [0 1 0 0 0]

      cat           = [0 0 1 0 0]

      dog          = [0 0 0 1 0]

      elephant = [0 0 0 0 1]

    * Word Class

      class 1 : dog, cat, bird

      class 2 : ran, jumped, walk

      class 3 : flower, tree, apple

    * Woed Embedding

      离散图

  * 概述

    * 机器通过阅读大量的文档而没有监督学习单词的意思
    * 通过内容可以理解一组词的意思
    * 可以从它所交往的人那里知道一组词
    * 无监督生成单词矢量

  * 方法

    * Count based

      如何两组词 wi 和 wj 以相同的频率出现，V(wi) 和 V(wj) 将彼此有关联

      V(wi) · V(wj)		~		Ni,j

      内积								相同的文档中出现大量的 wi 和 wj

    * Predition based

      一组词 wi-1 的 1-of-N encoding ~ 神经网络 ~ 每组词作为下一组词 wi 的可能性

      * 去掉第一层神经元的输入
      * 用它来代表一个单词 wi
      * 单词矢量，单词融合特征：V (wi)

    * Predition-based - Sharing Parameters

      * 同样颜色的重量应该是一样的

      * 一组词将有两个单词矢量

        ![avatar](./images/u121_Sharing_Parameters.png)

      * 如何让 wi 等于 wj
      * 给 wi 和 wj 相同的初始值

      $$
      w_i \leftarrow w_i - \eta \frac{\partial C}{\partial w_i} \\
      w_j \leftarrow w_j - \eta \frac{\partial C}{\partial w_j}
      $$

    * Predition-based - Trainning

    * Predition-based - Various Architectures

      ![avatar](./images/u121_Various_Architectures.png)

  * 其他方式

    * Multi-lingual Embedding
    * Muliti-domain Embedding
    * Document Embedding
    * Semantic Embedding

  * Beyond Bag of Word

    * Paragraph Vector
    * Seq2seq Auto-encoder
    * Skip Though

---

* **12.2 迁移学习 (Transformer Learning)**

  * Seq2seq model with "Self-attention"

  * Using CNN to replace RNN

  * Self-attention

    * 定义

      q : query (to match others)
      $$
      q^i = W^qa^i
      $$
      k : key (to be matched)
      $$
      k^i = W^ka^i
      $$
      v : information to be extracted
      $$
      v^i = W^va^i
      $$

    * 模型

      ![avatar](./images/u122_Self_attention_1.png)

      ![avatar](./images/u122_Self_attention_2.png)

      ![avatar](./images/u122_Self_attention_3.png)

      ![avatar](./images/u122_Self_attention_4.png)

      ![avatar](./images/u122_Self_attention_5.png)

      ![avatar](./images/u122_Self_attention_6.png)

  * Multi-head Self-attention

    ![avatar](./images/u122_Multi_head_Self_attention_1.png)

    ![avatar](./images/u122_Multi_head_Self_attention_2.png)

  * Positional Encoding

    * 在 self-attention 中没有位置信息
    * 原始论文 : 每个位置都有一个唯一的位置向量 e^i (不是从数据中学到的)
    * 换句话说，每个 x^i 加上一个热向量 p^i

  *  Seq2seq with Attention

    ![avatar](./images/u122_Seq2seq_with_Attention.png)

  * Transformer

    ![avatar](./images/u122_Transformer.png)

  * Attention Visualization
  * Universal Transformer

---

* **12.3  Unsupervised Learning - Linear Methods**

  * 化繁为简 Dimension Reduction

    * Clustering

      * K-means

        ![avatar](./images/u123_Clustering.png)

      * Hierarchical Agglomerative Clustering (HAC)

        ![avatar](./images/u123_HAC.png)

      * 降维 Dimension Reduction

        * 方法

          ![avatar](./images/u123_Dimension_Reduction_1.png)

          ![avatar](./images/u123_Dimension_Reduction_2.png)

        * PCA 推导

          ![avatar](./images/u123_PCA_1.png)

          ![avatar](./images/u123_PCA_2.png)

          ![avatar](./images/u123_PCA_3.png)

        * PCA - Another Point of View

          ![avatar](./images/u123_Another_Point_of_View_1.png)

          ![avatar](./images/u123_Another_Point_of_View_2.png)

          ![avatar](./images/u123_Another_Point_of_View_3.png)

        * PCA 特点

          ![avatar](./images/u123_PCA_Feature.png)

    * 因式分解 Matrix Factorization

      ![avatar](./images/u123_Matrix_Factorization.png)

  * 无中生有 Generation

---

* **12.4 Unsupervised Learning - Neighbor Embedding**

  * Mainfold Learning

    * 修剪或者遵循无监督学习是合适的

  * Locally Linear Embedding, LLE

    ![avatar](./images/u124_LLE_1.png)

    ![avatar](./images/u124_LLE_2.png)

  * Laplacian Eigenmaps

    ![avatar](./images/u124_Laplacian_Eigenmaps_1.png)

    ![avatar](./images/u124_Laplacian_Eigenmaps_2.png)

  * T-distributed Stochastic Neighbor Embedding (t-SNE)

    * 相似的数据接近，但是不同的是数据可能崩溃

    * 可视化很好

      ![avatar](./images/u124_t_SNE_1.png)

      ![avatar](./images/u124_t_SNE_2.png)

---

* **12.5 Unsupervised Learning - Deep Auto-encoder**

  * Auto-encoder

    ![avatar](./images/u125_Auto_encoder_1.png)

    ![avatar](./images/u125_Auto_encoder_2.png)

  * Deep Auto-encoder

    ![avatar](./images/u125_Deep_Auto_encoder.png)

  * Auto-encoder 应用
    * Text Retrieval
    * Similar Image Search
    * Pre-training DNN

  * Auto-encoder for CNN

    ![avatar](./images/u125_Auto_encoder_CNN_1.png)

    ![avatar](./images/u125_Auto_encoder_CNN_2.png)

    ![avatar](./images/u125_Auto_encoder_CNN_3.png)
    
  * More About Auto-encoder

    * 概述

      * 一个可嵌入的应该是一个对象

      * Beyond Reconstruction

        ![avatar](./images/u125_Beyond_Reconstruction.png)

      * Sequential Data

        ![avatar](./images/u125_Sequential_Data.png)

      * 功能区分 Feature Disentangle

        * 一个对象包含多个方面的信息
        * Feature Disentangle - Voice Conversion
          * 不同的人说相同的句子有不同的影响
          * 说话者分类器和编码器是迭代学习
        * Feature Disentangle - Designed Network Architecture
        * Feature Disentangle - Adversarial Training

      * 离散表示 Discrete Representation

        * 很容易解释和裁剪
        * Vector Quantized Variational Auto-encoder (VQVAE)

      * Sequence as Embedding

      * Tree as Embedding

    * More than minimizing reconstruction error

      * 使用 Discriminator
      * 连续的数据

    * More interpretable embedding

      * 功能区分
      * 离散和结构化

---

* **12.6 Unsupervised Learning - Deep Generation Model**

  * Pixel RNN

    ![avatar](./images/u126_Pixel_RNN.png)

  * Variational Auto Encoder, VAE

    ![avatar](./images/u126_VAE_1.png)

    ![avatar](./images/u126_VAE_2.png)

    ![avatar](./images/u126_VAE_3.png)

    ![avatar](./images/u126_VAE_4.png)

    ![avatar](./images/u126_VAE_5.png)

  * 生成对抗的网络 (Generative Adversarial Network, GAN)

    * The evolution of generation

      ![avatar](./images/u126_The_evolution_of_generation.png)

    * GAN - Discriminator

      ![avatar](./images/u126_GAN_Discriminator_1.png)

      ![avatar](./images/u126_GAN_Discriminator_2.png)

  * GAN 特性

    * GANs 很难优化
    * 没有明确的信号表明 generator 有多好
      * 在标准网络中，我们监测损失
      * 在 GANs 中， 我们必须保持“比赛中的 well-matched”
    * 当 discriminator 失败时，它不能保证这一点
      * 仅因为 discriminator  犯傻
      * 有时生成器会找到一个特定的例子，可能使 discriminator 失败
      * 使 discriminator  更健壮可能是有帮助的

