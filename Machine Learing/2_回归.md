#### 2.å›å½’ï¼ˆRegressionï¼‰

* ##### ä¸¾ä¾‹

  * è‚¡å¸‚é¢„æµ‹ (Stock Market Forecast)

  * è‡ªåŠ¨é©¾é©¶ (Self-driving Car)

  * å•†å“æ¨è (Recommendation)

    * f(x) = y

    * Example Application (pokemon)

      * x: x<sub>s</sub>(species), x<sub>hp</sub>(health value), x<sub>w</sub>(weight), x<sub>h</sub>(height)

      * y: combat power(cp) after evolution

        * Step 1: çº¿æ€§æ¨¡å‹ (Model)

          * y = b + w Â· x<sub>cp</sub>(most suitale) 
            $$
            y = b + \sum w_ih_i (Linear\ model)
            $$

        * Step 2 : æŸå¤±å‡½æ•° (Goodness of Function)

          * Training Data: 10 pokemon

            (x<sup>1</sup>, $\widehat{y}$<sup>1</sup>), (x<sup>2</sup>, $\widehat{y}$<sup>2</sup>), ......,  (x<sup>10</sup>, $\widehat{y}$<sup>10</sup>) 

            ![avatar](./images/line_regression_1.jpg)

          * Loss function L: bad

            $L(f) = L(w, b) = \sum_{n=1}^{10}(\widehat{y}^n - (b + w Â· x_{cp}^n))^2$

          * Best Function

            $f^* = arc\ min_{\{f\}} L(f)$

            $w^*, b ^* = arg\ min_{\{w,b\}} L(w, b) = arg\ min_{w,b}\sum_{n=1}^{10}(\widehat{y}^n - (b + w Â· x_{cp}^n))^2$

        * Step 3: æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent)

          * L : w

            * (Randomly) Pick an initial value w<sup>0</sup>

            * Compute $\frac{dL}{dw}|_{w=w^1}$

            * ......

          * L : w, b

            * (Randomly) Pick an initial value w<sup>0</sup>, b<sup>0</sup>

            * Compute $\frac{\partial{L}}{\partial{w}}|_{w=w^0,b=b^0}, \frac{\partial{L}}{\partial{b}}|_{w=w^0,b=b^0}$

            * ......

            * Results

              y = b + w Â· x<sub>cp</sub> (b = -188.4, w = 2.7)

              Average Error on Training Data = $\sum_{n=1}^{10}e^n = 31.9$

              Another 10 pokemons

            * Selecting another Model

              $y = b + w_1 Â· x_{cp} + w_2 Â· (x_{cp})^2 + w_3 Â· (x_{cp})^3 + ...$

              Compute ......

      * Reflection

        * Model Selection

          * Training
        * Testing
          * Overfitting 
      
        * Hidden factors

          * Species
        * Health value
      
        * Redesign

          If x<sub>s</sub> = Pidgey, $y = b_1 + w_1 Â· x_{cp}$

          If x<sub>s</sub> = Weedle, $y = b_1 + w_1 Â· x_{cp}$

          ......

        * Line regrassion

          ![avatar](./images/line_regression_2.jpg)

  ***

* ##### æ­¥éª¤

  * Modelï¼ˆç¡®å®šä¸€ä¸ªæ¨¡å‹ï¼‰â€”â€”çº¿æ€§æ¨¡å‹

  * Goodness of functionï¼ˆç¡®å®šè¯„ä»·å‡½æ•°ï¼‰â€”â€”æŸå¤±å‡½æ•°

  * Best functionï¼ˆæ‰¾å‡ºæœ€å¥½çš„ä¸€ä¸ªå‡½æ•°ï¼‰â€”â€”æ¢¯åº¦ä¸‹é™æ³•

    

  * Redesign the Model Again

  * Regularization (æ­£åˆ™åŒ–)

    * smooth functions (suitable)
    * less influence
    * $ \lambda $,  training, testing

  * Validation

    ![avatar](./images/conclusion_1.jpg)

***

* ##### æ¢¯åº¦ä¸‹é™ï¼ˆä¼˜åŒ–ï¼‰

  * æ¨¡å‹é€‰æ‹©

    * Review

    ![avatar](./images/review.jpg)

    * Estimator

      * $\widehat{y} = \widehat{f}(pokemon)$
      * Bias and Variance of Estimator
        * Estimate the mean of a variavle x
          * Assume the mean of x is $\mu$
          * Assume the variance of x is $\sigma$
        * Estimator of mean $\mu$
          * Sample N points: {x<sup>1</sup>, x<sup>2</sup>, ..., x<sup>N</sup>}

      $$
      \begin{align} \\
      &m = \frac{1}{N}\sum_n x^n \neq \mu \\
      &E[m] = E[\frac{1}{N}\sum_n x^n] = \frac{1}{N}\sum_n E[X^n] = \mu \\
      &Var[m] = \frac{\sigma}{N} (depen\ on\ the\ number\ of\ samples) \\ 
      \\ \\
      &m = \frac{1}{N}\sum_n x^n,\ s^2 = \frac{1}{N}\sum_n (x^n - m)^2 \\
      &E[s^2] = \frac{N-1}{N} \sigma^2
      \end{align}
      $$

    

    * Vriance

      Simpler model is less influenced by the sampled data

    * Bias

      $E[f^*] = \overline{y}$

      ![avatar](./images/bias_virance.jpg)

  * ä¼˜åŒ–

    * Review

      * $\theta^* = arg\ min_{\theta} L(\theta)$

        L: loss function

        $\theta$: parameters
        $$
        \Large \nabla L(\theta) = \left ( \begin{matrix} \frac{\partial{C(\theta_1)}}{\partial{\theta_1}} \\ \frac{\partial{C(\theta_2)}}{\partial{\theta_2}} \end{matrix} \right ) \\
        \Large \theta^1 = \theta^0 - \eta \nabla L(\theta^0) \\
        \Large \theta^2 = \theta^1 - \eta \nabla L(\theta^1)
        $$

      * åå¾®åˆ†æ³•

        ![avatar](./images/gradient_descent.jpg)

    * Learning Rate :  $\eta$

      * æ¯éš”å‡ ä»£é€šè¿‡ä¸€äº›å› ç´ å‡å°‘å­¦ä¹ é€Ÿç‡

      * ç»™ä¸åŒçš„å‚æ•°ä¸åŒçš„å­¦ä¹ é€Ÿç‡

      * å°†æ¯ä¸ªå‚æ•°çš„å­¦ä¹ é€Ÿç‡é™¤ä»¥å…¶å…ˆå‰å¯¼æ•°çš„å‡æ–¹æ ¹

        $\LARGE \eta^t = \frac{\eta}{\sqrt{t+1}}$

      * vanilla gradient descent æ‰¹æ¢¯åº¦ä¸‹é™

        $\LARGE w^{t+1} \leftarrow w^t \leftarrow \eta^tg^t$

      * Adagrad å°æ‰¹é‡éšæœºæ¢¯åº¦

        $\LARGE w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t}g^t$

        $\LARGE w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i-0}^t(g^i)^2}g^t}$

    * éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent)

      $\large L = \sum_n(\widehat{y}^n - (b+\sum w_i {x_i}^n))^2$

      * Gradient Descent

        $\large \theta^i = \theta^(i-1) - \eta \nabla L(\theta^(i-1))$

      * Stochastic Gradient Descent

        $\large L^n = \sum_n(\widehat{y}^n - (b+\sum w_i {x_i}^n))^2$

        $\large \theta^i = \theta^(i-1) - \eta \nabla L^n(\theta^(i-1))$

    * ç‰¹å¾ç¼©æ”¾ (Feature Scaling)

      $\large y = b + w_1x_1 + w_2x_2$

    * æ¨å¯¼

      ![avatar](./images/formal_derivation.jpg)

      ![avatar](./images/taylor_series_1.png)

      ![avatar](./images/taylor_series_2.png)

      ![avatar](./images/formal_derivation_2.jpg)

      ![avatar](./images/formal_derivation_3.jpg)

      ![avatar](./images/formal_derivation_4.jpg)

      ![avatar](./images/formal_derivation_5.jpg)

    * æ¢¯åº¦ä¸‹é™çš„æ›´å¤šé™åˆ¶

      ![avatar](./images/more_limitation.jpg)

***

* ##### æ–°ä¼˜åŒ–å™¨

  * èƒŒæ™¯çŸ¥è¯†

    * Î¼-å¼ºå‡¸æ€§ï¼ˆÎ¼-strong convexityï¼‰

    * åˆ©æ™®å¸­èŒ¨è¿ç»­æ€§ ï¼ˆLipschitz continuityï¼‰

    * å¸ƒé›·æ ¼æ›¼ä¸ç­‰å¼ ï¼ˆBregman proximal inequalityï¼‰

    * å‚æ•°

      $\theta_t$ : æ—¶é—´æ­¥æ¨¡å‹å‚æ•° t

      $\nabla L(\theta_t)\ or\ g_t$ : å¡åº¦ $\theta_t$ï¼Œç”¨äºè®¡ç®— $\theta_{t+1}$

      $m_{t+1}$ : ä»æ—¶é—´æ­¥é•¿ 0 åˆ°æ—¶é—´æ­¥é•¿ç´¯ç§¯çš„åŠ¨é‡ tï¼Œç”¨æ¥è®¡ç®— $\theta_{t+1}$

      ![avatar](./images/parameters.png)

    * ä¼˜åŒ–å†…å®¹
      * æ‰¾åˆ°ä¸€ä¸ª ğœƒ å¾—åˆ°æœ€ä½çš„ $\sum_x L(\theta;x)$
      * æ‰¾ä¸€ä¸ª ğœƒ å¾—åˆ°æœ€ä½çš„ ğ¿(ğœƒ)
    * On-line vs Off-line
      * On-lineï¼šone pair of $(x_t, \widehat{y}_t)$ at a time step
      * Off-lineï¼špour all $(x_t, \widehat{y}_t)$ into the model at  every time step

  * **SGD (Stochastic Gradient Descent éšæœºæ¢¯åº¦ä¸‹é™)**

    ![avatar](./images/sgd_1.png)

  * **SGDM  (Stochastic Gradient Descent with Momentum å¸¦åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™)**

  ![avatar](./images/sgdm_1.png)

  ![avatar](./images/sgdm_2.png)

  ![avatar](./images/sgdm_3.png)

  * **Adagrad ç®—æ³•**

    ![avatar](./images/Adagrad_1.png)

  * **RMSProp ç®—æ³•**

    ![avatar](./images/RMSProp.png)

  * **Adam ç®—æ³•**

    ![avatar](./images/Adam.png)

  * SWATS

    ![avatar](./images/SWATS.png)

  * Towards Improving Adamâ€¦

    * AMSGrad
    * AdaBound

  * Towards Improving SGDMâ€¦

    * LR range test
    * Cyclical LR
    * SGDR
    * One-cycle LR

***

* ##### ç»“è®º

  * Diagnosis
    * If your model cannot even fit the training examples, then you have large bias ( (Underfitting æ¬ æ‹Ÿåˆ)
    * If you can fit the training data, but large error on testing data, then you probaly have large variance  (Overfitting è¿‡æ‹Ÿåˆ)
  * For bias, redesign your model
    * Add more feature as input
    * A more comolex model
  * For variance
    * More data : Very effective, but not always practical
    * Regularization











































