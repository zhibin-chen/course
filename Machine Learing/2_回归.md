#### 2.回归（Regression）

* ##### 2.1 举例

  * 股市预测 (Stock Market Forecast)

  * 自动驾驶 (Self-driving Car)

  * 商品推荐 (Recommendation)

    * f(x) = y

    * Example Application (pokemon)

      * x: x<sub>s</sub>(species), x<sub>hp</sub>(health value), x<sub>w</sub>(weight), x<sub>h</sub>(height)

      * y: combat power(cp) after evolution

        * Step 1: 线性模型 (Model)

          * y = b + w · x<sub>cp</sub>(most suitale) 
            $$
            y = b + \sum w_ih_i (Linear\ model)
            $$

        * Step 2 : 损失函数 (Goodness of Function)

          * Training Data: 10 pokemon

            (x<sup>1</sup>, $\widehat{y}$<sup>1</sup>), (x<sup>2</sup>, $\widehat{y}$<sup>2</sup>), ......,  (x<sup>10</sup>, $\widehat{y}$<sup>10</sup>) 

            ![avatar](./images/u21_line_regression_1.jpg)

          * Loss function L: bad

            $L(f) = L(w, b) = \sum_{n=1}^{10}(\widehat{y}^n - (b + w · x_{cp}^n))^2$

          * Best Function

            $f^* = arc\ min_{\{f\}} L(f)$

            $w^*, b ^* = arg\ min_{\{w,b\}} L(w, b) = arg\ min_{w,b}\sum_{n=1}^{10}(\widehat{y}^n - (b + w · x_{cp}^n))^2$

        * Step 3: 梯度下降法 (Gradient Descent)

          * L : w

            * (Randomly) Pick an initial value w<sup>0</sup>

            * Compute $\frac{dL}{dw}|_{w=w^1}$

            * ......

          * L : w, b

            * (Randomly) Pick an initial value w<sup>0</sup>, b<sup>0</sup>

            * Compute $\frac{\partial{L}}{\partial{w}}|_{w=w^0,b=b^0}, \frac{\partial{L}}{\partial{b}}|_{w=w^0,b=b^0}$

            * ......

            * Results

              y = b + w · x<sub>cp</sub> (b = -188.4, w = 2.7)

              Average Error on Training Data = $\sum_{n=1}^{10}e^n = 31.9$

              Another 10 pokemons

            * Selecting another Model

              $y = b + w_1 · x_{cp} + w_2 · (x_{cp})^2 + w_3 · (x_{cp})^3 + ...$

              Compute ......

      * Reflection

        * Model Selection

          * Training
        * Testing
          * Overfitting 
      
        * Hidden factors

          * Species
        * Health value
      
        * Redesign

          If x<sub>s</sub> = Pidgey, $y = b_1 + w_1 · x_{cp}$

          If x<sub>s</sub> = Weedle, $y = b_1 + w_1 · x_{cp}$

          ......

        * Line regrassion

          ![avatar](./images/u21_line_regression_2.jpg)

---

* ##### 2.2 步骤

  * Model（确定一个模型）——线性模型

  * Goodness of function（确定评价函数）——损失函数

  * Best function（找出最好的一个函数）——梯度下降法

    

  * Redesign the Model Again

  * Regularization (正则化)

    * smooth functions (suitable)
    * less influence
    * $ \lambda $,  training, testing

  * Validation

    ![avatar](./images/u22_conclusion.jpg)

***

* ##### 2.3 梯度下降（优化）

  * 模型选择

    * Review

    ![avatar](./images/u23_review.jpg)

    * Estimator

      * $\widehat{y} = \widehat{f}(pokemon)$
      * Bias and Variance of Estimator
        * Estimate the mean of a variavle x
          * Assume the mean of x is $\mu$
          * Assume the variance of x is $\sigma$
        * Estimator of mean $\mu$
          * Sample N points: {x<sup>1</sup>, x<sup>2</sup>, ..., x<sup>N</sup>}

      $$
      \begin{align} \\
      &m = \frac{1}{N}\sum_n x^n \neq \mu \\
      &E[m] = E[\frac{1}{N}\sum_n x^n] = \frac{1}{N}\sum_n E[X^n] = \mu \\
      &Var[m] = \frac{\sigma}{N} (depen\ on\ the\ number\ of\ samples) \\ 
      \\ \\
      &m = \frac{1}{N}\sum_n x^n,\ s^2 = \frac{1}{N}\sum_n (x^n - m)^2 \\
      &E[s^2] = \frac{N-1}{N} \sigma^2
      \end{align}
      $$

    

    * Vriance

      Simpler model is less influenced by the sampled data

    * Bias

      $E[f^*] = \overline{y}$

      ![avatar](./images/u23_bias_virance.jpg)

  * 优化

    * Review

      * $\theta^* = arg\ min_{\theta} L(\theta)$

        L: loss function

        $\theta$: parameters
        $$
        \Large \nabla L(\theta) = \left ( \begin{matrix} \frac{\partial{C(\theta_1)}}{\partial{\theta_1}} \\ \frac{\partial{C(\theta_2)}}{\partial{\theta_2}} \end{matrix} \right ) \\
        \Large \theta^1 = \theta^0 - \eta \nabla L(\theta^0) \\
        \Large \theta^2 = \theta^1 - \eta \nabla L(\theta^1)
        $$

      * 偏微分法

        ![avatar](./images/u23_gradient_descent.jpg)

    * Learning Rate :  $\eta$

      * 每隔几代通过一些因素减少学习速率

      * 给不同的参数不同的学习速率

      * 将每个参数的学习速率除以其先前导数的均方根

        $\LARGE \eta^t = \frac{\eta}{\sqrt{t+1}}$

      * vanilla gradient descent 批梯度下降

        $\LARGE w^{t+1} \leftarrow w^t \leftarrow \eta^tg^t$

      * Adagrad 小批量随机梯度

        $\LARGE w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t}g^t$

        $\LARGE w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i-0}^t(g^i)^2}g^t}$

    * 随机梯度下降 (Stochastic Gradient Descent)

      $\large L = \sum_n(\widehat{y}^n - (b+\sum w_i {x_i}^n))^2$

      * Gradient Descent

        $\large \theta^i = \theta^(i-1) - \eta \nabla L(\theta^(i-1))$

      * Stochastic Gradient Descent

        $\large L^n = \sum_n(\widehat{y}^n - (b+\sum w_i {x_i}^n))^2$

        $\large \theta^i = \theta^(i-1) - \eta \nabla L^n(\theta^(i-1))$

    * 特征缩放 (Feature Scaling)

      $\large y = b + w_1x_1 + w_2x_2$

    * 推导

      ![avatar](./images/u23_formal_derivation_1.jpg)

      ![avatar](./images/u23_formal_derivation_2.png)

      ![avatar](./images/u23_formal_derivation_3.png)

      ![avatar](./images/u23_formal_derivation_4.jpg)

      ![avatar](./images/u23_formal_derivation_5.jpg)

      ![avatar](./images/u23_formal_derivation_6.jpg)

      ![avatar](./images/u23_formal_derivation_7.jpg)

    * 梯度下降的更多限制

      ![avatar](./images/u23_more_limitation.jpg)

***

* ##### 2.4 新优化器

  * 背景知识

    * μ-强凸性（μ-strong convexity）

    * 利普席茨连续性 （Lipschitz continuity）

    * 布雷格曼不等式 （Bregman proximal inequality）

    * 参数

      $\theta_t$ : 时间步模型参数 t

      $\nabla L(\theta_t)\ or\ g_t$ : 坡度 $\theta_t$，用于计算 $\theta_{t+1}$

      $m_{t+1}$ : 从时间步长 0 到时间步长累积的动量 t，用来计算 $\theta_{t+1}$

      ![avatar](./images/u24_parameters.png)

    * 优化内容
      * 找到一个 𝜃 得到最低的 $\sum_x L(\theta;x)$
      * 找一个 𝜃 得到最低的 𝐿(𝜃)
    * On-line vs Off-line
      * On-line：one pair of $(x_t, \widehat{y}_t)$ at a time step
      * Off-line：pour all $(x_t, \widehat{y}_t)$ into the model at  every time step

  * **SGD (Stochastic Gradient Descent 随机梯度下降)**

    $\large \theta_t = \theta_{t-1} - \nabla L_{l_2}(\theta_{t-1}) = \theta_{t-1} - \nabla L(\theta_{t-1}) - \gamma \theta_{t-1}$

    ![avatar](./images/u24_sgd.png)

  * **SGDM  (Stochastic Gradient Descent with Momentum 带动量的随机梯度下降)**

    $\large \theta_t = \theta_{t-1} - m_t$

    $\large m_t = \lambda m_{t-1} + \eta \nabla L(\theta_{t-1})$

  ![avatar](./images/u24_sgdm_1.png)

  ![avatar](./images/u24_sgdm_2.png)

  ![avatar](./images/u24_sgdm_3.png)

  * **Adagrad 算法**

    ![avatar](./images/u24_Adagrad.png)

  * **RMSProp 算法**

    ![avatar](./images/u24_RMSProp.png)

  * **Adam 算法**

    ![avatar](./images/u24_Adam.png)

  * SWATS

    ![avatar](./images/u24_SWATS.png)

  * Towards Improving Adam…

    * AMSGrad

      $\LARGE \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\widehat{v}_t}+\sigma}$

      $\LARGE \widehat{v}_t = max(\widehat{v}_{t-1}, v_t)$

    * AdaBound

      $\LARGE \theta_t = \theta_{t-1} - Clip(\frac{\eta}{\sqrt{\widehat{v}_t+\sigma}}) \widehat{m}_t$

      $\LARGE Clip(x) = Clip(x, 0.1 - \frac{0.1}{(1-\beta_2)t+1}, 0.1 + \frac{0.1}{1-\beta_2}t)$

  * Towards Improving SGDM…

    * LR range test
    
    * Cyclical LR
    
    * SGDR
    
    * One-cycle LR
    
      warm-up + annealing + fine-tuning
    
      RAdam
    
      ![avatar](./images/u24_radam_vs_swats.png)
    
  * Lookahead

    * 所有优化器的通用包装器
    * 稳定

  * Nesterov accelerated gradient (NAG)

    * SGDM

      $\large \theta_t = \theta_{t-1} - m_t$

      $\large m_t = \lambda m_{t-1} + \eta \nabla L(\theta_{t-1})$

    * 超前部署

      $\large \theta_t = \theta_{t-1} - m_t$

      $\large m_t = \lambda m_{t-1} + \eta \nabla L(\theta_{t-1} - \lambda m_{t-1})$

  * Nadam

    $\LARGE \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\widehat{v}_t}+\varepsilon} \widehat{m}_t$

    $\LARGE \widehat{m}_t = \frac{\beta_1 m_t}{1-\beta_1^{t+1}} + \frac{(1-\beta_1)g_{t-1}}{1-\beta_1^t}$

  * AdamW & SGDW with momentum

    * SGDWM

      $\large \theta_t = \theta_{t-1} - m_t - \gamma \theta_{t-1}$

      $\large m_t = \lambda m_{t-1} + \eta(\nabla L(\theta_{t-1}))$

    * AdamW

      $\large m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla(\theta_{t-1})$

      $\large v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla L(\theta_{t-1}))$^2

      $\large \theta_t = \theta_{t-1} - \eta(\frac{1}{\sqrt{\widehat{v}_t}+\varepsilon}\widehat{m}_t + \gamma \theta_{t-1})$

***

* ##### 2.5 结论

  * 测试及优化

    * Diagnosis
      * 模型不能适应训练的例子，会有很大的偏差 (Underfitting 欠拟合)
      * 模型能拟合训练数据，但是测试数据误差很大，有很大的方差  (Overfitting 过拟合)
    * 重新设计模型来减少误差
      * 添加更多功能作为输入
      * 更复杂的模型
    * 减少方差
      * 更多数据：非常有效，但并不总是实用
      * 正则化

  * 有助于优化

    * Shuffling

    * Dropout

    * Gradient noise

      $\large g_{t,i} = g_{t,i} + N(0,\sigma_t^2)$

      $\large \sigma_t = \frac{c}{(1+t)\gamma}$

    * Warm-up
    * Curriculum learning
    * Fine-tuning
    * Normalization 规范化
    * Regularization 正则化

  * 比较

    ![avatar](./images/u25_campare_1.png)

    ![avatar](./images/u25_campare_2.png)

    ![avatar](./images/u25_campare_3.png)

