#### 2.回归（Regression）

* 举例

  * 股市预测 (Stock Market Forecast)

  * 自动驾驶 (Self-driving Car)

  * 商品推荐 (Recommendation)

    * f(x) = y

    * Example Application (pokemon)

      * x: x<sub>s</sub>(species), x<sub>hp</sub>(health value), x<sub>w</sub>(weight), x<sub>h</sub>(height)

      * y: combat power(cp) after evolution

        * Step 1: 线性模型 (Model)

          * y = b + w · x<sub>cp</sub>(most suitale) 
            $$
            y = b + \sum w_ih_i (Linear\ model)
            $$

        * Step 2 : 损失函数 (Goodness of Function)

          * Training Data: 10 pokemon

            (x<sup>1</sup>, $\widehat{y}$<sup>1</sup>), (x<sup>2</sup>, $\widehat{y}$<sup>2</sup>), ......,  (x<sup>10</sup>, $\widehat{y}$<sup>10</sup>) 
            
            ![avatar](./images/line_regression_1.jpg)

          * Loss function L: bad

            $L(f) = L(w, b) = \sum_{n=1}^{10}(\widehat{y}^n - (b + w · x_{cp}^n))^2$

          * Best Function

            $f^* = arc\ min_{\{f\}} L(f)$

            $w^*, b ^* = arg\ min_{\{w,b\}} L(w, b) = arg\ min_{w,b}\sum_{n=1}^{10}(\widehat{y}^n - (b + w · x_{cp}^n))^2$

        * Step 3: 梯度下降法 (Gradient Descent)

          * L : w

            * (Randomly) Pick an initial value w<sup>0</sup>

            * Compute $\frac{dL}{dw}|_{w=w^0}$

              Negative -> Increase w

              Positive -> Decrease w*

            * Compute $\frac{dL}{dw}|_{w=w^1}$

            * ......

          * L : w, b

            * (Randomly) Pick an initial value w<sup>0</sup>, b<sup>0</sup>
            * Compute $\frac{\partial{L}}{\partial{w}}|_{w=w^0,b=b^0}, \frac{\partial{L}}{\partial{b}}|_{w=w^0,b=b^0}$

            * ......

            * Results

              y = b + w · x<sub>cp</sub> (b = -188.4, w = 2.7)

              Average Error on Training Data = $\sum_{n=1}^{10}e^n = 31.9$

              Another 10 pokemons

            * Selecting another Model

              $y = b + w_1 · x_{cp} + w_2 · (x_{cp})^2 + w_3 · (x_{cp})^3 + ...$

              Compute ......

      * Reflection

        * Model Selection

          * Training
          * Testing
          * Overfitting 

        * Hidden factors

          * Species
          * Health value

        * Redesign

          If x<sub>s</sub> = Pidgey, $y = b_1 + w_1 · x_{cp}$

          If x<sub>s</sub> = Weedle, $y = b_1 + w_1 · x_{cp}$

          ......

        * Line regrassion

          ![avatar](./images/line_regression_2.jpg)

* 步骤

  * Model（确定一个模型）——线性模型

  * Goodness of function（确定评价函数）——损失函数

  * Best function（找出最好的一个函数）——梯度下降法

    

  * Redesign the Model Again

  * Regularization (正则化)

    * smooth functions (suitable)
    * less influence
    * $ \lambda $,  training, testing

  * Validation

    ![avatar](./images/conclusion_1.jpg)

* 重点

  * 模型选择

    * Review

    ![avatar](./images/review.jpg)

    * Estimator

      * $\widehat{y} = \widehat{f}(pokemon)$
      * Bias and Variance of Estimator
        * Estimate the mean of a variavle x
          * Assume the mean of x is $\mu$
          * Assume the variance of x is $\sigma$
        * Estimator of mean $\mu$
          * Sample N points: {x<sup>1</sup>, x<sup>2</sup>, ..., x<sup>N</sup>}

      $$
      \begin{align} \\
      &m = \frac{1}{N}\sum_n x^n \neq \mu \\
      &E[m] = E[\frac{1}{N}\sum_n x^n] = \frac{1}{N}\sum_n E[X^n] = \mu \\
      &Var[m] = \frac{\sigma}{N} (depen\ on\ the\ number\ of\ samples) \\ 
      \\ \\
      &m = \frac{1}{N}\sum_n x^n,\ s^2 = \frac{1}{N}\sum_n (x^n - m)^2 \\
      &E[s^2] = \frac{N-1}{N} \sigma^2
      \end{align}
      $$

    

    * Vriance

      Simpler model is less influenced by the sampled data

    * Bias

      $E[f^*] = \overline{y}$

      ![avatar](./images/bias_virance.jpg)

  * 梯度下降法（优化）

    * Review

      * $\theta^* = arg\ min_{\theta} L(\theta)$

        L: loss function

        $\theta$: parameters

      * 偏微分法

        ![avatar](./images/gradient_descent.jpg)

    * Learning Rate

      * 

* 结论

  * Diagnosis
    * If your model cannot even fit the training examples, then you have large bias ( (Underfitting 欠拟合)
    * If you can fit the training data, but large error on testing data, then you probaly have large variance  (Overfitting 过拟合)
  * For bias, redesign your model
    * Add more feature as input
    * A more comolex model
  * For variance
    * More data : Very effective, but not always practical
    * Regularization











































