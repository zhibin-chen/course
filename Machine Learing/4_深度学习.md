#### 4.深度学习（Deep Learning）

* ##### 4.1 简介

  * 步骤

    * 神经网络 Neural network
    * 函数优度 Goodness of function
    * 选择最佳函数 Pick the best function

  * 说明

    * 全连接前馈网络 Fully Connect Feedforward Network

      ![avatar](./images/u41_fully_connect_1.png)

      ![avatar](./images/u41_fully_connect_2.png)

    * Deep = Many hidden layers

    * 矩阵运算 Matrix Operation

      ![avatar](./images/u41_matrix_operation.png)

    * 神经网络

      ![avatar](./images/u41_neural_network.png)

    * 输出层作为多类分类器 Output Layer as Multi-Class Classifier

      ![avatar](./images/u41_output_layer.png)

    * Example

      ![avatar](./images/u41_example_1.png)

      ![avatar](./images/u41_example_2.png)

    * 梯度下降 Gradient Descent

      ![avatar](./images/u41_gradient_descent.png)

---

* **4.2 反向传播算法 Backpropagation**

  * 梯度下降 Gradient Descent

    ![avatar](./images/u42_gradient_descent.png)

  * 链式法则 Chain Rule

    ![avatar](./images/u42_chain_rule.png)

  * 反向传播

    * 简介

      ![avatar](./images/u42_backpropagation_1.png)

      ![avatar](./images/u42_backpropagation_2.png)

    * Forward Pass

      ![avatar](./images/u42_forward_pass.png)

    * Backward Pass

      ![avatar](./images/u42_backward_pass_1.png)

      ![avatar](./images/u42_backward_pass_2.png)

      ![avatar](./images/u42_backward_pass_3.png)

    * 总结

      ![avatar](./images/u42_sumarry.png)

---

* **4.3 深度学习的技巧 Tips for Deep Learning**
  * Recipe of Deep Learning

    ![avatar](./images/u43_recipe.png)

  * 消除梯度问题

    * 整流线性单元 ReLU (Rectified Linear Unit)

      ![avatar](./images/u43_ReLU.png)

    * 最大输出 Maxout

      * ReLU 是特殊的 Maxout

      * 学习激活函数 Learnable activation function

        * 在最大输出网络中的激活函数可以是任何分段线性凸函数
        * 多少件取决于一组中有多少元素

        ![avatar](./images/u43_Maxout_1.png)

        ![avatar](./images/u43_Maxout_2.png)

        ![avatar](./images/u43_Maxout_3.png)

  * Review

    * Adagrad

      ![avatar](./images/u43_Adagrad.png)

    * RMSProp

      ![avatar](./images/u43_RMSProp.png)

    * Vanilla Gradient Descent

      ![avatar](./images/u43_Vanilla.png)

    * Momentum

      ![avatar](./images/u43_Momentum_1.png)

      ![avatar](./images/u43_Momentum_2.png)

    * Adam (RMSProp + Momentum)

      ![avatar](./images/u43_Adam.png)

  * 正则化 Regularization

    ![avatar](./images/u43_Regularization_1.png)

    ![avatar](./images/u43_Regularization_2.png)

    ![avatar](./images/u43_Regularization_3.png)

  * Dropout

    * Training 每次更新参数前

      * 每个神经元有 p% 的下降 (网络结构发生变化)
      * 使用新网络进行培训
      * 对于每一个小批次，我们重新取样辍学神经元

    * Testing, no dropout

      * 如果在训练中退出率是 p%，那么所有的比重率是 1-p%
      * 假设退出率是50%，如果训练权重 w = 1，设置 w = 0.5 进行测试

    * 原因

      * 当团队合作时，如果每个人都希望合作伙伴完成工作，那么最终什么也做不了
      * 然而，如果你知道你的伙伴将退出，你会做得更好
      * 测试时，实际上没有人中途退出，所以最终获得了良好的结果

    * 退出率是一套整体

      ![avatar](./images/u43_Dropout_1.png)

      ![avatar](./images/u43_Dropout_2.png)

  * 模块化 Modularization

    * Deep ~ Modularization

    * 根据特征分类（更多层）

    * 再分类（少数据，性能好，模块化从数据中自动学习）

      

    * 语音应用 Speech
  
      * 音素 Phoneme
        * 两位音
        * 三位音
        * 状态 state
      * 语音识别的第一阶段
        * 分类
          * 输入：声学特性
          * 输出：状态
        * 每个状态都有一个平稳的声学特征分布
      * 高斯混合模型 Gaussian Mixture Model （GMM）
        * 并列状态 Tied-state
      * 在 HMM-GMM 中，所有的音素都是独立建模的
        * 不是模仿人声的有效方法
        * 发音被一些因素控制
        * 所有的状态都使用相同的 DNN

---

* **4.4 深度学习的优势**
  * 普适性定理

    * 任意连续函数 f：$R^N \rightarrow R^M$
    * 可以通过一个带有一个隐含层的网络来实现 （给定足够的隐藏层）
    * 浅网络可以表示任何函数，然而，使用深层结构是更有效的

  * 类比 Analogy

    ![avatar](./images/u44_Analogy.png)

  * 端到端学习 End-to-end Learning

    * 每个函数都可以自动学习
    * 语音识别
      * 波形 - DFT- 光谱图 - 滤波器组 - 记录 - DCT - MFCC - GMM - "Hello"
      * 函数模拟处理（信号与系统）

  * 复杂任务

    * 相似输入，不同输出
    * 不同输入，相似输出
    * 多层 hidden learn

