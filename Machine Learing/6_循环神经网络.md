#### 6.循环神经网络（Recurrent Neural Network, RNN）

* ##### 6.1 示例应用程序

  * Slot Filling

    * 方法
      * 1-of-N encoding
      * Beyond 1-of-N encoding
    * Using Feedforward network
      * 输入：一组词 （每组词都可以用一个矢量代替）
      * 输出：输入字属于插槽的概率分布
  * Elman Network & Jordan Network
  * Bidirectional RNN

---

* **6.2 Long Short-term Memory (LSTM)**

  * 性质

    * 可以处理渐变消失(不是渐变爆炸)

    * 添加了内存和输入

    * 除非忘记门关闭，否则这种影响永远不会消失

    * 没有渐变消失(如果忘记门被打开)

    * 门循环单元(Gated Recurrent Unit, GRU) : 比 LSTM 简单

      ![avatar](./images/u62_LSTM_1.png)

      ![avatar](./images/u62_LSTM_2.png)

      ![avatar](./images/u62_LSTM_3.png)

  * 有用的技术

    * Clockwise RNN
    * Structurally Constrained Recurrent Network (SCRN)

---

* **6.3 RNN**

  * 特点

    * 隐层的输出存储在存储器中

    * 内存可以看作是另一种输入

      

    *  Many to One :  输入是矢量序列，但输出是一个矢量

    * Many to Many :

      * 输入和输出都是矢量序列，但输出更短 (CTC)
      * 输入和输出都是不同长度的矢量序列，序列到序列学习
      * 没有限制

  * 应用

    * Beyond Sequence

      * Syntactic parsing

    * Sequence-to-sequence Auto-encoder 

      * Text

      * Speech

        

      * RNN Encoder

      * RNN Decoder

    * Demo 

      * Chat-bot
      * Video Caption Generation
      * Image Caption Generation

    * Attention-based Model

    * Reading Comprehension

      * Visual Question Answering

      * Model Architecture

        ![avatar](./images/u63_Example.png)

  * RNN vs Structured Learning

    * 比较

      ![avatar](./images/u63_RNN_vs_Structured_Learning.png)

    * 结合

      * 语音识别：CNN/LSTM/DNN + HMM

        ![avatar](./images/u63_Speech_Recognition.png)

      * 语义标记：Bi-directional LSTM + CRF/Structured SVM

        ![avatar](./images/u63_Semantic_Tagging.png)
    
  * Conditional GAN
  
    * 将能量模型与GAN连接
      * 生成对抗网络、逆强化学习和能量模型之间的连接
      * 能量模型概率估计的深度定向生成模型
      * 基于能量生成的对抗网络
    * 用于推理的深度学习模型
      * 深度展开 : 基于模型的新深度建筑灵感
      * 作为循环神经网络的条件随机场

---

* **6.4 Conditional Generation by RNN & Attention**

  * Generation

    * 组件的组件生成一个结构体对象 
      * 句子：每次使用 RNN 生成一个字母/字
      * 图片：每次使用 RNN 生成一个像素
      * 语音
      * 影像
    * Conditional Generation
      * 将输入条件表示为一个矢量，考虑该矢量作为 RNN 生成器的输入
        * Encoder
        * Decoder

  * Attention

    * 动态条件生成

    * Machine Translation 

      * Attention-based model

        ![avatar](./images/u64_Attention_based_model_1.png)

        ![avatar](./images/u64_Attention_based_model_2.png)

    * Speech Recognation

    * Image Caption Generation

    * Memory Network：阅读回答问题

      ![avatar](./images/u64_Memory_Network.png)

    * 神经图灵机 Neural Turing Machine

      ![avatar](./images/u64_Neural_Turing_Machine_1.png)

      ![avatar](./images/u64_Neural_Turing_Machine_2.png)

  * Tips for Generation

    * Good Attention：每个输入组件都有大约相同的 attetion weight

    * Mismatch between Train and Test

      ![avatar](./images/u64_Mismatch_between_Train_and_Test_1.png)

      ![avatar](./images/u64_Mismatch_between_Train_and_Test_2.png)

    * 方法

      * 安排抽样 Scheduled Sampling

        ![avatar](./images/u64_Scheduled_Sampling.png)

      * 定向搜索 Beam Search

        ![avatar](./images/u64_Beam_Search.png)

    * Object level v.s. Component level

      * 最小化组件级定义的错误并不等同于改进生成的对象

      * 解决方法：Reinforcement learning

        ![avatar](./images/u64_Object_level_vs_Component_level.png)

  * Pointer Network

    ![avatar](./images/u64_Pointer_Network.png)

